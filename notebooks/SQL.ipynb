{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Definitions & setup logic\n",
    "# ────────────────────────────────────────────────────\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Compute project paths\n",
    "nb_dir   = Path.cwd().resolve()    # …/notebooks\n",
    "repo_dir = nb_dir.parent           # repo root\n",
    "\n",
    "# Make sure our repo root is on the import path *before* any pipeline imports\n",
    "sys.path.insert(0, str(repo_dir))\n",
    "\n",
    "# Now it will find pipeline.utils.duckdb_wrapper\n",
    "from pipeline.utils.duckdb_wrapper import DuckDBWrapper\n",
    "from pipeline.datasets import *\n",
    "\n",
    "# Helper: initialize a DuckDBWrapper with optional reset logic\n",
    "def initialize(duckdb_path: Path, delete_on_disk: bool, reset_catalog: bool):\n",
    "    \"\"\"\n",
    "    1) Closes any existing `con`\n",
    "    2) Deletes on‐disk files (duckdb + WAL) if delete_on_disk=True\n",
    "    3) Connects a fresh DuckDBWrapper at duckdb_path\n",
    "    4) Drops all tables/views in‐session if reset_catalog=True\n",
    "    Returns the new `con`.\n",
    "    \"\"\"\n",
    "    global con\n",
    "\n",
    "    # 1) close old connection\n",
    "    try:\n",
    "        con.con.close()\n",
    "        del con\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # 2) delete on-disk files if asked\n",
    "    if delete_on_disk:\n",
    "        wal_file = duckdb_path.with_suffix(duckdb_path.suffix + \"-wal\")\n",
    "        for f in (duckdb_path, wal_file):\n",
    "            if f.exists():\n",
    "                f.unlink()\n",
    "\n",
    "    # 3) connect\n",
    "    duckdb_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    con = DuckDBWrapper(duckdb_path)\n",
    "\n",
    "    # 4) reset in-session catalog if asked\n",
    "    if reset_catalog:\n",
    "        rows = con.con.execute(\"\"\"\n",
    "            SELECT table_name, table_type\n",
    "            FROM information_schema.tables\n",
    "            WHERE table_schema = 'main'\n",
    "        \"\"\").fetchall()\n",
    "        for name, typ in rows:\n",
    "            con.con.execute(f'DROP {typ} IF EXISTS \"{name}\" CASCADE')\n",
    "\n",
    "    return con\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to: ../data/duckdb/test.duckdb\n",
      "Current tables/views:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">     Registered Tables     </span>\n",
       "┏━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Table Name </span>┃<span style=\"font-weight: bold\"> Table Type </span>┃\n",
       "┡━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "└────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m     Registered Tables     \u001b[0m\n",
       "┏━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTable Name\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTable Type\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "└────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Cell 2: Configure & initialize\n",
    "\n",
    "# 1) DuckDB file path (edit if you move it)\n",
    "duckdb_file  = Path(\"../data/duckdb/test.duckdb\")  \n",
    "\n",
    "# 2) One toggle for a “fresh start”\n",
    "FRESH_START  = True   # if True: deletes on‐disk + resets catalog; if False: leaves everything intact\n",
    "\n",
    "# 3) Run initialization (passes the same flag twice)\n",
    "con = initialize(\n",
    "    duckdb_path    = duckdb_file,\n",
    "    delete_on_disk = FRESH_START,\n",
    "    reset_catalog  = FRESH_START,\n",
    ")\n",
    "\n",
    "# 4) Inspect\n",
    "print(f\"Connected to: {duckdb_file}\")\n",
    "print(\"Current tables/views:\")\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "View 'nfl_pbp_2024' created for files at '/home/christiandata/nfl-analytics-pipeline/data/opendata/clean/nfl_pbp_2024/*.parquet'.\n",
      "View 'nfl_pbp_ten_years' created for files at '/home/christiandata/nfl-analytics-pipeline/data/opendata/clean/nfl_pbp_ten_years/*.parquet'.\n",
      "Skipping partitioned : no .parquet files found with pattern => /home/christiandata/nfl-analytics-pipeline/data/opendata/clean/year=*/month=*/*.parquet\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #008000; text-decoration-color: #008000; font-weight: bold\">        Registered Tables         </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Table Name        </span>┃<span style=\"font-weight: bold\"> Table Type </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000000; font-weight: bold\"> nfl_pbp_2024      </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000000; font-weight: bold\"> VIEW       </span>│\n",
       "├───────────────────┼────────────┤\n",
       "│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000000; font-weight: bold\"> nfl_pbp_ten_years </span>│<span style=\"color: #c0c0c0; text-decoration-color: #c0c0c0; background-color: #000000; font-weight: bold\"> VIEW       </span>│\n",
       "└───────────────────┴────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;32m        Registered Tables         \u001b[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mTable Name       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mTable Type\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━┩\n",
       "│\u001b[1;37;40m \u001b[0m\u001b[1;37;40mnfl_pbp_2024     \u001b[0m\u001b[1;37;40m \u001b[0m│\u001b[1;37;40m \u001b[0m\u001b[1;37;40mVIEW      \u001b[0m\u001b[1;37;40m \u001b[0m│\n",
       "├───────────────────┼────────────┤\n",
       "│\u001b[1;37;40m \u001b[0m\u001b[1;37;40mnfl_pbp_ten_years\u001b[0m\u001b[1;37;40m \u001b[0m│\u001b[1;37;40m \u001b[0m\u001b[1;37;40mVIEW      \u001b[0m\u001b[1;37;40m \u001b[0m│\n",
       "└───────────────────┴────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_PATH = \"data/opendata/clean\"\n",
    "\n",
    "con.bulk_register_data(\n",
    "    repo_root   = repo_dir,\n",
    "    base_path   = BASE_PATH,\n",
    "    table_names = SINGLE_FILE_ASSETS_NAMES,\n",
    "    wildcard    = \"*.parquet\",\n",
    "    as_table    = False,\n",
    ")\n",
    "\n",
    "con.bulk_register_partitioned_data(\n",
    "    repo_root   = repo_dir,\n",
    "    base_path   = BASE_PATH,\n",
    "    table_names = PARTITIONED_ASSETS_NAMES,\n",
    "    wildcard    = \"year=*/month=*/*.parquet\",\n",
    "    as_table    = False,\n",
    "    show_tables = True,                 # optional: prints a neat summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = f\"\"\"\n",
    "SELECT \n",
    "    week,\n",
    "    penalty_type,\n",
    "    COUNT(*) AS total_penalties\n",
    "FROM nfl_pbp_2024\n",
    "WHERE penalty_type IS NOT NULL\n",
    "GROUP BY \n",
    "    week,\n",
    "    penalty_type\n",
    "ORDER BY \n",
    "    week ASC\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query)\n",
    "\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If you want a better looking table, set show_results=True. I'd recomend capping the limit at about 50 rows\n",
    "\n",
    "\n",
    "query = f\"\"\"\n",
    "\n",
    "SELECT \n",
    "    week,\n",
    "    penalty_type,\n",
    "    COUNT(*) AS total_penalties\n",
    "FROM nfl_pbp_2024\n",
    "WHERE penalty_type IS NOT NULL\n",
    "GROUP BY \n",
    "    week,\n",
    "    penalty_type\n",
    "ORDER BY \n",
    "    week ASC\n",
    "\n",
    "LIMIT 20\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query,show_results=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (576, 3)\n",
      "┌──────┬───────────────────────────────┬─────────────────┐\n",
      "│ week ┆ penalty_type                  ┆ total_penalties │\n",
      "│ ---  ┆ ---                           ┆ ---             │\n",
      "│ i32  ┆ str                           ┆ i64             │\n",
      "╞══════╪═══════════════════════════════╪═════════════════╡\n",
      "│ 1    ┆ Delay of Game                 ┆ 13              │\n",
      "│ 1    ┆ Illegal Contact               ┆ 2               │\n",
      "│ 1    ┆ Neutral Zone Infraction       ┆ 2               │\n",
      "│ 1    ┆ Kick Catch Interference       ┆ 1               │\n",
      "│ 1    ┆ Illegal Shift                 ┆ 3               │\n",
      "│ …    ┆ …                             ┆ …               │\n",
      "│ 22   ┆ Offensive Holding             ┆ 3               │\n",
      "│ 22   ┆ Illegal Block Above the Waist ┆ 1               │\n",
      "│ 22   ┆ False Start                   ┆ 3               │\n",
      "│ 22   ┆ Illegal Use of Hands          ┆ 1               │\n",
      "│ 22   ┆ Unsportsmanlike Conduct       ┆ 1               │\n",
      "└──────┴───────────────────────────────┴─────────────────┘\n",
      "File written to: /home/christiandata/nfl-analytics-pipeline/data/exports/penalty_totals.csv\n"
     ]
    }
   ],
   "source": [
    "#export one off query queries into parquet, CSV, or JSON\n",
    "query = f\"\"\"\n",
    "\n",
    "SELECT \n",
    "    week,\n",
    "    penalty_type,\n",
    "    COUNT(*) AS total_penalties\n",
    "FROM nfl_pbp_2024\n",
    "WHERE penalty_type IS NOT NULL\n",
    "GROUP BY \n",
    "    week,\n",
    "    penalty_type\n",
    "ORDER BY \n",
    "    week ASC\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = con.run_query(query)\n",
    "\n",
    "print(result)\n",
    "\n",
    "repo_root = Path.cwd().resolve().parents[0]  # Adjust to locate the repo root\n",
    "base_path = repo_root / \"data/exports\"\n",
    "file_name = \"penalty_totals\"\n",
    "file_type= \"csv\"\n",
    "# Export the query result to CSV\n",
    "con.export(result, file_type=file_type, base_path=base_path, file_name=file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute and export all .sql files in the notebooks/sql folder\n",
    "sql_dir    = nb_dir / \"sql\"                  # notebooks/sql/*.sql\n",
    "export_dir = repo_dir / \"data\" / \"exports\"   # where we write .parquet\n",
    "export_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for sql_path in sorted(sql_dir.glob(\"*.sql\")):\n",
    "    sql_text  = sql_path.read_text()\n",
    "    out_name  = sql_path.stem                # e.g. rides, new, test\n",
    "    print(f\"▶︎ Running {sql_path.name}\")\n",
    "\n",
    "    result_df = con.run_query(sql_text)      # Polars DataFrame\n",
    "\n",
    "    con.export(\n",
    "        result     = result_df,\n",
    "        file_type  = \"parquet\",\n",
    "        base_path  = export_dir,\n",
    "        file_name  = out_name,\n",
    "    )\n",
    "\n",
    "print(f\"✓ Done — results saved under {export_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optional:register your new outputs into duckdb\n",
    "result_paths  = sorted(export_dir.glob(\"*.parquet\"))\n",
    "view_names    = [p.stem for p in result_paths]          # \"new\", \"rides\", \"test\", …\n",
    "\n",
    "con.register_data_view(\n",
    "    paths        = result_paths,\n",
    "    table_names  = view_names\n",
    ")\n",
    "\n",
    "# optional: confirm they’re there\n",
    "con.show_tables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Close the notebook before using Harlequin with the created duckdb file\n",
    "con.con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------\n",
    "# Convert every notebooks/sql/*.sql into dbt‑compatible models\n",
    "# ---------------------------------------------------------------------\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "DEBUG = False                  # set True to enable convertdbtsql.py \"debug\" mode\n",
    "\n",
    "# 1) locate repo root\n",
    "nb_dir   = Path.cwd().resolve()           # .../notebooks\n",
    "repo_dir = nb_dir.parent                  # repo root\n",
    "\n",
    "# 2) build all required paths\n",
    "sql_dir   = nb_dir / \"sql\"                # source folder (already exists)\n",
    "dest_dir  = repo_dir / \"transformations\" / \"dbt\" / \"models\"\n",
    "tool_path = repo_dir / \"tools\" / \"convertdbtsql.py\"\n",
    "\n",
    "# 3) sanity checks\n",
    "for p in (sql_dir, tool_path):\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "dest_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 4) assemble argv and run\n",
    "argv = [\n",
    "    sys.executable,                       # the current Python interpreter\n",
    "    str(tool_path),\n",
    "    str(dest_dir),\n",
    "    str(sql_dir),\n",
    "]\n",
    "if DEBUG:\n",
    "    argv.append(\"debug\")\n",
    "\n",
    "print(\"▶︎ Converting SQL files for dbt …\")\n",
    "subprocess.run(argv, check=True)\n",
    "print(f\"✓ Done — rewritten models are in {dest_dir}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
